# Promtail configuration for Docker container log collection
# Collects logs from Docker containers via Docker logging driver
# Extracts trace IDs for Grafana/Tempo integration

server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  # Docker container discovery - automatically discovers and collects logs via Docker API
  # CRITICAL: Using Docker API directly (not file tailing) eliminates filesystem buffering delays
  # This ensures logs are captured in real-time without the 5-10 second delay from file I/O buffering
  # By NOT setting __path__ to a file path, Promtail uses Docker's API to stream logs directly
  #
  # Note: No name filters - relabel_configs below handle categorization more robustly
  # This ensures we don't accidentally miss logs from new services
  - job_name: easyflow-services
    docker_sd_configs:
      - host: unix:///var/run/docker.sock
        refresh_interval: 5s

    relabel_configs:
      # Extract container name (remove leading slash)
      - source_labels: [__meta_docker_container_name]
        regex: '/(.*)'
        target_label: container_name
        replacement: '${1}'

      # Default job name (use container name) - MUST come first
      # This sets a default job label for all containers, which can then be overwritten
      # by more specific rules below. This ensures consistent labeling.
      - source_labels: [__meta_docker_container_name]
        regex: '/(.*)'
        target_label: job
        replacement: '${1}'

      # Map container names to job names for better organization
      # These specific rules run AFTER the default rule, so they can overwrite it
      # Matches various naming patterns: easy-flow-backend-1, easyflow-backend, rpa-system-backend, backend
      # Note: Docker container names include leading slash, so "backend" becomes "/backend"
      - source_labels: [__meta_docker_container_name]
        regex: '.*(easy-flow-backend|easyflow-backend|rpa-system-backend|/backend$).*'
        target_label: job
        replacement: 'easyflow-backend'

      - source_labels: [__meta_docker_container_name]
        regex: '.*(easy-flow-rpa-dashboard|easyflow-frontend|rpa-dashboard).*'
        target_label: job
        replacement: 'easyflow-frontend'

      - source_labels: [__meta_docker_container_name]
        regex: '.*(easy-flow-automation-worker|automation-worker|automation-service).*'
        target_label: job
        replacement: 'easyflow-automation'

      # Set service label based on job
      - source_labels: [job]
        target_label: service
        replacement: '${1}'

      # Set environment label
      - target_label: environment
        replacement: 'development'

      # CRITICAL: Do NOT set __path__ to a file path
      # By omitting __path__, Promtail uses Docker's API directly to stream logs
      # This eliminates filesystem buffering delays and provides real-time log capture
      # The docker_sd_configs automatically provides the correct target for Docker API log streaming

    pipeline_stages:
      # Stage 0: Parse Docker's JSON log format
      # Docker wraps each log line in JSON: {"log":"<application log>","stream":"stdout","time":"<timestamp>"}
      # Extract Docker wrapper fields AND preserve them for timestamp parsing
      - json:
          expressions:
            docker_log: log
            docker_stream: stream
            docker_time: time

      # Stage 1: Parse application log JSON without discarding Docker fields
      # Parse the docker_log field as JSON to extract application log fields
      # CRITICAL: Using source: docker_log preserves docker_time field from Stage 0
      # Rename application timestamps to app_time and app_timestamp to avoid conflicts
      - json:
          source: docker_log
          expressions:
            level: level
            msg: msg
            app_time: time
            app_timestamp: timestamp
            service: service
            logger: logger
            trace_id: trace.traceId
            span_id: trace.spanId
            user_id: trace.userId
            method: trace.method
            path: trace.path
            execution_id: error.execution_id
            workflow_id: error.workflow_id

      # Stage 2: Regex fallback - Extract trace ID if JSON parsing didn't find it
      # This handles non-JSON logs or logs where traceId is in a different format
      # Looks for any 32-character hexadecimal string (standard trace ID format)
      - regex:
          expression: '(?P<trace_id_fallback>[0-9a-fA-F]{32})'
          source: msg

      # Merge: Use JSON-extracted trace_id if available, otherwise use regex fallback
      - template:
          source: trace_id
          template: '{{ if .trace_id }}{{ .trace_id }}{{ else if .trace_id_fallback }}{{ .trace_id_fallback }}{{ end }}'

      # Stage 3: Trace stage - Extract trace_id as internal field (label) for Loki
      # This tells Loki to treat trace_id as a trace identifier, making it discoverable by Grafana
      # The label enables fast filtering: {trace_id="75ae2874a1cf402204aa76b473332b6f"}
      #
      # CRITICAL: Do NOT extract 'service' label here - it's already set in relabel_configs
      # If application log doesn't contain service field, this would overwrite the correct
      # service label with empty value, causing Loki indexing issues and "no logs found" errors
      - labels:
          level:
          logger:
          trace_id:

      # Stage 4: Timestamp handling - Ensure logs have correct timestamps
      # CRITICAL FIX: Use Docker's timestamp as primary source (most accurate)
      #
      # Priority order (stops at first successful parse):
      # 1. Docker's timestamp (RFC3339Nano) - Most reliable, represents actual log capture time
      # 2. Application timestamp field (RFC3339 ISO format from Pino) - Fallback if Docker time missing
      # 3. Application time field (UnixMs - milliseconds) - Fallback if timestamp field missing
      #
      # Note: Promtail processes timestamp stages sequentially. If a timestamp stage fails,
      # it continues to the next one. We use 'skip' to prevent overwriting a successfully parsed timestamp.
      - timestamp:
          source: docker_time
          format: RFC3339Nano
          action_on_failure: skip
      - timestamp:
          source: app_timestamp
          format: RFC3339
          action_on_failure: skip
      - timestamp:
          source: app_time
          format: UnixMs
          action_on_failure: skip

      # Stage 5: Output formatting - Use original Docker log content with traceId appended
      # CRITICAL: Use docker_log (original application log) instead of reconstructing from msg
      # This preserves the original log structure while using the accurate Docker timestamp
      # Append traceId to the log content for Grafana derived fields to extract
      - template:
          source: output
          template: '{{ .docker_log }}{{ if .trace_id }} traceId="{{ .trace_id }}"{{ end }}'

      - output:
          source: output

  # PM2 log file collection - collects logs from PM2-managed services
  # These services run outside Docker, so we need to tail their log files
  - job_name: easyflow-pm2-logs
    static_configs:
      # Backend logs
      - targets:
          - localhost
        labels:
          job: easyflow-backend
          service: rpa-system-backend
          environment: development
          __path__: /app/logs/backend.log
          __path_exclude__: /app/logs/backend-error.log

      # Frontend logs
      - targets:
          - localhost
        labels:
          job: easyflow-frontend
          service: rpa-dashboard
          environment: development
          __path__: /app/logs/frontend.log
          __path_exclude__: /app/logs/frontend-error.log

      # Automation worker logs
      - targets:
          - localhost
        labels:
          job: easyflow-automation
          service: automation-service
          environment: development
          __path__: /app/logs/automation-worker.log

    pipeline_stages:
      # CRITICAL: Strip PM2 timestamp prefix before JSON parsing
      # PM2 adds timestamp prefix like "2025-12-16 22:10:24: " before JSON logs
      # This regex extracts just the JSON part after the timestamp
      - regex:
          expression: '^\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}:\s+(?P<json_line>.*)$'
          source: message
      
      # Use extracted JSON line if available, otherwise use original message
      - template:
          source: message
          template: '{{ if .json_line }}{{ .json_line }}{{ else }}{{ .message }}{{ end }}'
      
      # Parse JSON logs from PM2 (Pino logger format)
      - json:
          expressions:
            level: level
            msg: msg
            time: time
            timestamp: timestamp
            service: service
            logger: logger
            trace_id: trace.traceId
            span_id: trace.spanId
            user_id: trace.userId
            method: trace.method
            path: trace.path
            execution_id: error.execution_id
            workflow_id: error.workflow_id

      # Extract trace ID via regex fallback if JSON parsing didn't find it
      - regex:
          expression: '(?P<trace_id_fallback>[0-9a-fA-F]{32})'
          source: msg

      # Merge: Use JSON-extracted trace_id if available, otherwise use regex fallback
      - template:
          source: trace_id
          template: '{{ if .trace_id }}{{ .trace_id }}{{ else if .trace_id_fallback }}{{ .trace_id_fallback }}{{ end }}'

      # Extract labels for filtering
      - labels:
          level:
          logger:
          trace_id:

      # Parse timestamps (Pino uses UnixMs format)
      - timestamp:
          source: timestamp
          format: RFC3339
          action_on_failure: skip
      - timestamp:
          source: time
          format: UnixMs
          action_on_failure: skip

      # Output the log line
      - output:
          source: msg
