services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: easyflow
      POSTGRES_PASSWORD: easyflow
      POSTGRES_DB: easyflow
    volumes:
      - pgdata:/var/lib/postgresql/data
    ports:
      - '5432:5432'
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U easyflow -d easyflow"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7
    ports:
      - '6379:6379'
    volumes:
      - redisdata:/data

  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:7.3.0
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      # Use host.docker.internal for Docker Desktop, fallback to host IP
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://kafka:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT_INTERNAL
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_INTERNAL://0.0.0.0:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    ports:
      - '9092:9092'
    depends_on:
      - zookeeper

  backend:
    build:
      context: ./rpa-system/backend
      dockerfile: Dockerfile
    env_file:
      - ./rpa-system/backend/.env
    # For local development we enable a safe bind-mount pattern that preserves the
    # container-installed `node_modules` while allowing live edits of source files.
    volumes:
      - ./rpa-system/backend:/usr/src/app:delegated
      - backend_node_modules:/usr/src/app/node_modules
    ports:
      - '3000:3030'
    depends_on:
      - postgres
      - redis
      - kafka
    environment:
      DATABASE_URL: "postgres://easyflow:easyflow@postgres:5432/easyflow"
      REDIS_URL: "redis://redis:6379"
      KAFKA_BROKERS: "kafka:29092"
      KAFKA_BOOTSTRAP_SERVERS: "kafka:29092"
      AUTOMATION_URL: "http://automation-worker:5000"
      ALLOW_DRAFT_EXECUTION: 'true'
      KAFKA_ENABLED: 'true'

  automation-worker:
    build:
      context: ./rpa-system/automation
      dockerfile: automation-service/Dockerfile
    volumes:
      - ./rpa-system/automation/automation-service:/app
    ports:
      - '7001:5000'
    depends_on:
      - backend
      - kafka
      - postgres
    environment:
      AUTOMATION_WORKER_MODE: local
      BACKEND_URL: "http://backend:3030"
      KAFKA_BROKERS: kafka:29092
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      PORT: "5000"
      # Supabase for file uploads - loaded from .env file
      SUPABASE_URL: "${SUPABASE_URL}"
      SUPABASE_KEY: "${SUPABASE_KEY}"
      SUPABASE_SERVICE_ROLE: "${SUPABASE_SERVICE_ROLE}"

  rpa-dashboard:
    build:
      context: ./rpa-system/rpa-dashboard
    env_file:
      - ./rpa-system/rpa-dashboard/.env.local
    ports:
      - '3000:3000'
    volumes:
      - ./rpa-system/rpa-dashboard:/usr/src/app
      - dashboard_node_modules:/usr/src/app/node_modules
    depends_on:
      - backend

  chroma:
    image: ghcr.io/chroma-core/chroma:latest
    environment:
      - CHROMA_DB_IMPL=duckdb+parquet
    ports:
      - '8000:8000'
    volumes:
      - chroma_data:/var/lib/chroma

  rag-service:
    build:
      context: ./services/rag
      dockerfile: Dockerfile
    env_file:
      - ./services/rag/.env
    ports:
      - '3002:3002'
    depends_on:
      - chroma
    environment:
      LLM_API_URL: "${LLM_API_URL}"
      LANGSMITH_API_URL: "${LANGSMITH_API_URL}"
      LANGSMITH_API_KEY: "${LANGSMITH_API_KEY}"

volumes:
  pgdata:
  redisdata:
  backend_node_modules:
  dashboard_node_modules:
  chroma_data:
