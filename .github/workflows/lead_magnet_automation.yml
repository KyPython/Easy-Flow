# Lead Magnet Automation Workflow
# ================================
# 
# PURPOSE: Fully autonomous production-grade system that:
# - Collects Reddit feedback and pain points every Monday at 6 AM UTC
# - Generates targeted PDF checklists based on user frustrations
# - Analyzes qualitative + quantitative data for product insights
# - Updates marketing messaging and commits all changes automatically
# - Sends comprehensive notifications to Slack and Discord
#
# RELIABILITY FEATURES:
# - Comprehensive caching for dependencies and data
# - Exponential backoff retry logic for API calls
# - Graceful error handling with partial failure recovery
# - Idempotent operations that can be safely re-run
# - Detailed logging for production monitoring
#
# ENVIRONMENT REQUIREMENTS:
# - Python 3.11 with scientific computing stack
# - Node.js 20 for any frontend tooling
# - Git configuration for automated commits
# - Webhook URLs for notifications (stored as secrets)

name: ðŸŽ¯ Lead Magnet Automation Pipeline

# TRIGGER CONFIGURATION
# Runs every Monday at 6:00 AM UTC (customize timezone as needed)
# Also allows manual triggering for testing and emergency runs
on:
  schedule:
    # Monday 6:00 AM UTC (cron: minute hour day-of-month month day-of-week)
    - cron: '0 6 * * 1'
  
  # Manual trigger with optional parameters for customization
  workflow_dispatch:
    inputs:
      reddit_keywords:
        description: 'Reddit search keywords (comma-separated)'
        required: false
        default: 'automation,self-hosting,business-process,rpa,workflow'
      subreddits:
        description: 'Target subreddits (comma-separated)'
        required: false
        default: 'smallbusiness,entrepreneur,selfhosted,webdev,sysadmin,automation'
      force_regenerate:
        description: 'Force regenerate all checklists (ignore cache)'
        required: false
        default: 'false'
        type: boolean
      skip_notifications:
        description: 'Skip Slack/Discord notifications'
        required: false
        default: 'false'
        type: boolean

# WORKFLOW PERMISSIONS
# Ensure the workflow can read repository content and write back changes
permissions:
  contents: write
  actions: read
  security-events: read

# ENVIRONMENT VARIABLES
# Global configuration used across all jobs
env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '20'
  REDDIT_KEYWORDS: ${{ github.event.inputs.reddit_keywords || 'automation,self-hosting,business-process,rpa,workflow' }}
  TARGET_SUBREDDITS: ${{ github.event.inputs.subreddits || 'smallbusiness,entrepreneur,selfhosted,webdev,sysadmin,automation' }}
  FORCE_REGENERATE: ${{ github.event.inputs.force_regenerate || 'false' }}
  SKIP_NOTIFICATIONS: ${{ github.event.inputs.skip_notifications || 'false' }}

# MAIN JOB DEFINITION
jobs:
  lead_magnet_automation:
    name: ðŸš€ Generate Lead Magnets & Analyze Feedback
    runs-on: ubuntu-latest
    timeout-minutes: 30  # Prevent runaway jobs
    
    # JOB-LEVEL ENVIRONMENT VARIABLES
    # Set up paths and configuration for the automation scripts
    env:
      DATA_DIR: ./data
      SCRIPTS_DIR: ./scripts
      CHECKLISTS_DIR: ./public/downloads/checklists
      FEEDBACK_FILE: ./data/feedback/survey_responses.json
      OUTPUT_DIR: ./data/exports
    
    steps:
      # STEP 1: REPOSITORY SETUP
      # Check out the repository with full git history for proper commit attribution
      - name: ðŸ“¥ Checkout Repository
        uses: actions/checkout@v4
        with:
          # Fetch full history to enable proper git operations
          fetch-depth: 0
          # Use a personal access token if needed for private repos
          token: ${{ secrets.GITHUB_TOKEN }}
      
      # STEP 2: PYTHON ENVIRONMENT SETUP
      # Configure Python 3.11 with comprehensive dependency caching
      - name: ðŸ Set up Python Environment
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          # Enable pip caching for faster subsequent runs
          cache: 'pip'
      
      # STEP 3: NODE.JS ENVIRONMENT SETUP
      # Set up Node.js for any frontend build steps or tooling
      - name: ðŸŸ¢ Set up Node.js Environment
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          # Enable npm caching for faster dependency installation
          cache: 'npm'
          cache-dependency-path: '**/package-lock.json'
      
      # STEP 4: SYSTEM DEPENDENCIES
      # Install system-level dependencies required for PDF generation and NLP
      - name: ðŸ“¦ Install System Dependencies
        run: |
          echo "ðŸ”§ Installing system dependencies for PDF generation..."
          sudo apt-get update
          sudo apt-get install -y \
            fonts-liberation \
            fonts-dejavu-core \
            fontconfig \
            libfreetype6-dev \
            libjpeg-dev \
            libpng-dev \
            zlib1g-dev
          
          echo "âœ… System dependencies installed successfully"
      
      # STEP 5: PYTHON DEPENDENCIES WITH CACHING
      # Install Python packages with retry logic and comprehensive caching
      - name: ðŸ“š Install Python Dependencies
        run: |
          echo "ðŸ”„ Installing Python dependencies with caching..."
          
          # Create requirements file if it doesn't exist
          if [ ! -f requirements.txt ]; then
            echo "ðŸ“ Creating requirements.txt..."
            cat > requirements.txt << EOF
          praw>=7.7.1
          textblob>=0.17.1
          reportlab>=4.0.4
          requests>=2.31.0
          pandas>=2.1.0
          nltk>=3.8.1
          python-dotenv>=1.0.0
          beautifulsoup4>=4.12.2
          lxml>=4.9.3
          Pillow>=10.0.0
          EOF
          fi
          
          # Install with retry logic
          for i in {1..3}; do
            echo "ðŸ“¦ Installation attempt $i/3..."
            if pip install --upgrade pip && pip install -r requirements.txt; then
              echo "âœ… Python dependencies installed successfully"
              break
            elif [ $i -eq 3 ]; then
              echo "âŒ Failed to install dependencies after 3 attempts"
              exit 1
            else
              echo "âš ï¸ Installation failed, retrying in 10 seconds..."
              sleep 10
            fi
          done
          
          # Download NLTK data for text analysis
          python -c "
          import nltk
          try:
              nltk.download('punkt', quiet=True)
              nltk.download('vader_lexicon', quiet=True)
              nltk.download('stopwords', quiet=True)
              print('âœ… NLTK data downloaded successfully')
          except Exception as e:
              print(f'âš ï¸ NLTK download warning: {e}')
          "
      
      # STEP 6: DIRECTORY STRUCTURE SETUP
      # Create necessary directories with proper permissions
      - name: ðŸ“ Initialize Directory Structure
        run: |
          echo "ðŸ—ï¸ Setting up directory structure..."
          
          # Create all required directories
          mkdir -p "$DATA_DIR"/{feedback,exports,cache}
          mkdir -p "$CHECKLISTS_DIR"
          mkdir -p logs
          
          # Initialize empty feedback file if it doesn't exist
          if [ ! -f "$FEEDBACK_FILE" ]; then
            echo "[]" > "$FEEDBACK_FILE"
            echo "ðŸ“ Initialized empty feedback file"
          fi
          
          # Set proper permissions
          chmod -R 755 "$SCRIPTS_DIR"
          chmod -R 755 "$DATA_DIR"
          
          echo "âœ… Directory structure initialized"
          ls -la "$DATA_DIR"
      
      # STEP 7: REDDIT FEEDBACK COLLECTION
      # Run Reddit pain point extraction with comprehensive error handling
      - name: ðŸ” Extract Reddit Pain Points
        id: reddit_extraction
        run: |
          echo "ðŸŽ¯ Starting Reddit feedback extraction..."
          
          # Set up environment variables for the script
          export REDDIT_CLIENT_ID="${{ secrets.REDDIT_CLIENT_ID }}"
          export REDDIT_CLIENT_SECRET="${{ secrets.REDDIT_CLIENT_SECRET }}"
          export REDDIT_USER_AGENT="EasyFlow-LeadMagnet-Bot:v2.0 (by /u/easyflow_automation)"
          
          # Create cache directory for Reddit data
          CACHE_FILE="$DATA_DIR/cache/reddit_cache_$(date +%Y%m%d).json"
          OUTPUT_FILE="$DATA_DIR/reddit_pain_points.json"
          
          # Check if we have recent cached data (less than 6 hours old)
          if [ "$FORCE_REGENERATE" != "true" ] && [ -f "$CACHE_FILE" ] && [ $(($(date +%s) - $(stat -c %Y "$CACHE_FILE"))) -lt 21600 ]; then
            echo "ðŸ“‹ Using cached Reddit data from today..."
            cp "$CACHE_FILE" "$OUTPUT_FILE"
            echo "reddit_cached=true" >> $GITHUB_OUTPUT
          else
            echo "ðŸ”„ Fetching fresh Reddit data..."
            
            # Run extraction with retry logic
            for attempt in {1..3}; do
              echo "ðŸ“¡ Reddit extraction attempt $attempt/3..."
              
              if python "$SCRIPTS_DIR/fetch_reddit_feedback.py" \
                --keywords "$REDDIT_KEYWORDS" \
                --subreddits "$TARGET_SUBREDDITS" \
                --output "$OUTPUT_FILE" \
                --limit 200; then
                
                # Cache successful result
                cp "$OUTPUT_FILE" "$CACHE_FILE"
                echo "âœ… Reddit extraction completed successfully"
                echo "reddit_cached=false" >> $GITHUB_OUTPUT
                break
                
              elif [ $attempt -eq 3 ]; then
                echo "âŒ Reddit extraction failed after 3 attempts"
                echo "reddit_success=false" >> $GITHUB_OUTPUT
                
                # Use fallback mock data to continue workflow
                python "$SCRIPTS_DIR/fetch_reddit_feedback.py" \
                  --keywords "$REDDIT_KEYWORDS" \
                  --output "$OUTPUT_FILE" \
                  --limit 10 || exit 1
                
                echo "âš ï¸ Using mock data as fallback"
                break
              else
                echo "ðŸ”„ Attempt failed, waiting 30 seconds before retry..."
                sleep 30
              fi
            done
          fi
          
          # Validate output file
          if [ ! -f "$OUTPUT_FILE" ] || [ ! -s "$OUTPUT_FILE" ]; then
            echo "âŒ Reddit extraction produced no output"
            exit 1
          fi
          
          # Extract metrics for reporting
          PAIN_POINTS_COUNT=$(python -c "
          import json
          try:
              with open('$OUTPUT_FILE') as f:
                  data = json.load(f)
              count = sum(len(points) for points in data.get('pain_points', {}).values())
              print(count)
          except:
              print(0)
          ")
          
          echo "pain_points_count=$PAIN_POINTS_COUNT" >> $GITHUB_OUTPUT
          echo "ðŸ“Š Extracted $PAIN_POINTS_COUNT pain points from Reddit"
      
      # STEP 8: CHECKLIST GENERATION
      # Generate PDF checklists with professional formatting and error recovery
      - name: ðŸ“‹ Generate Lead Magnet Checklists
        id: checklist_generation
        run: |
          echo "ðŸ“„ Starting checklist generation..."
          
          INPUT_FILE="$DATA_DIR/reddit_pain_points.json"
          
          # Verify input file exists
          if [ ! -f "$INPUT_FILE" ]; then
            echo "âŒ Reddit pain points file not found: $INPUT_FILE"
            exit 1
          fi
          
          # Run checklist generation with comprehensive error handling
          for attempt in {1..2}; do
            echo "ðŸŽ¨ Checklist generation attempt $attempt/2..."
            
            if python "$SCRIPTS_DIR/generate_checklists.py" \
              --input "$INPUT_FILE" \
              --output-dir "$CHECKLISTS_DIR" \
              --format pdf; then
              
              echo "âœ… Checklists generated successfully"
              break
              
            elif [ $attempt -eq 2 ]; then
              echo "âŒ PDF generation failed, trying text format fallback..."
              
              # Fallback to text format if PDF generation fails
              python "$SCRIPTS_DIR/generate_checklists.py" \
                --input "$INPUT_FILE" \
                --output-dir "$CHECKLISTS_DIR" \
                --format text || exit 1
              
              echo "âš ï¸ Generated text checklists as fallback"
            else
              echo "ðŸ”„ Generation failed, retrying in 15 seconds..."
              sleep 15
            fi
          done
          
          # Count generated files
          CHECKLIST_COUNT=$(find "$CHECKLISTS_DIR" -name "*.pdf" -o -name "*.txt" | wc -l)
          echo "checklist_count=$CHECKLIST_COUNT" >> $GITHUB_OUTPUT
          echo "ðŸ“Š Generated $CHECKLIST_COUNT checklist files"
          
          # List generated files for verification
          echo "ðŸ“‚ Generated files:"
          ls -la "$CHECKLISTS_DIR" || true
      
      # STEP 9: FEEDBACK ANALYSIS
      # Analyze combined Reddit and survey data to generate actionable insights
      - name: ðŸ“Š Analyze Feedback & Generate Insights
        id: feedback_analysis
        run: |
          echo "ðŸ§  Starting comprehensive feedback analysis..."
          
          REDDIT_DATA="$DATA_DIR/reddit_pain_points.json"
          SURVEY_DATA="$FEEDBACK_FILE"
          ANALYSIS_OUTPUT="$DATA_DIR/feedback_analysis.json"
          CSV_OUTPUT_DIR="$OUTPUT_DIR"
          
          # Ensure survey data file exists
          if [ ! -f "$SURVEY_DATA" ]; then
            echo "[]" > "$SURVEY_DATA"
            echo "ðŸ“ Created empty survey data file"
          fi
          
          # Run analysis with error handling
          if python "$SCRIPTS_DIR/analyze_feedback.py" \
            --reddit-data "$REDDIT_DATA" \
            --survey-data "$SURVEY_DATA" \
            --output "$ANALYSIS_OUTPUT" \
            --csv-dir "$CSV_OUTPUT_DIR"; then
            
            echo "âœ… Feedback analysis completed successfully"
            
            # Extract key metrics from analysis
            INSIGHTS_COUNT=$(python -c "
            import json
            try:
                with open('$ANALYSIS_OUTPUT') as f:
                    data = json.load(f)
                insights = data.get('insights', {})
                count = len(insights.get('top_pain_points', [])) + len(insights.get('feature_recommendations', []))
                print(count)
            except:
                print(0)
            ")
            
            echo "insights_count=$INSIGHTS_COUNT" >> $GITHUB_OUTPUT
            echo "ðŸ“ˆ Generated $INSIGHTS_COUNT actionable insights"
            
          else
            echo "âŒ Feedback analysis failed"
            echo "insights_count=0" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          # Verify output files
          echo "ðŸ“‚ Analysis outputs:"
          ls -la "$OUTPUT_DIR" || true
          ls -la "$ANALYSIS_OUTPUT" || true
      
      # STEP 10: MESSAGING UPDATE
      # Update marketing messaging based on latest feedback insights
      - name: ðŸ“ Update Marketing Messaging
        id: messaging_update
        run: |
          echo "âœï¸ Updating marketing messaging based on insights..."
          
          ANALYSIS_FILE="$DATA_DIR/feedback_analysis.json"
          MESSAGING_FILE="./messaging.md"
          
          # Check if analysis file exists and has content
          if [ ! -f "$ANALYSIS_FILE" ] || [ ! -s "$ANALYSIS_FILE" ]; then
            echo "âš ï¸ No analysis data available, skipping messaging update"
            echo "messaging_updated=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # Create backup of current messaging
          if [ -f "$MESSAGING_FILE" ]; then
            cp "$MESSAGING_FILE" "${MESSAGING_FILE}.backup"
            echo "ðŸ’¾ Created backup of current messaging"
          fi
          
          # Update messaging timestamp and add analysis summary
          python -c "
          import json
          from datetime import datetime
          
          # Load analysis data
          try:
              with open('$ANALYSIS_FILE') as f:
                  analysis = json.load(f)
              
              insights = analysis.get('insights', {})
              summary = insights.get('executive_summary', {})
              
              # Read current messaging
              try:
                  with open('$MESSAGING_FILE') as f:
                      content = f.read()
              except:
                  content = '# EasyFlow Marketing Messaging Strategy\n\n*Generated by automated feedback analysis*\n'
              
              # Update footer with latest analysis
              footer = f'''
          ---
          
          ## ðŸ¤– Latest Analysis Summary
          
          **Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M UTC')}  
          **Total Feedback Points:** {summary.get('total_feedback_points', 0)}  
          **Reddit Pain Points:** {summary.get('reddit_pain_points', 0)}  
          **Survey Responses:** {summary.get('survey_responses', 0)}  
          
          **Top Pain Points:**
          '''
              
              for i, pain in enumerate(insights.get('top_pain_points', [])[:3], 1):
                  footer += f'\\n{i}. **{pain.get('category', 'Unknown').title()}** (severity: {pain.get('severity', 0):.1f})'
              
              footer += '\\n\\n**Top Feature Requests:**'
              
              for i, feature in enumerate(insights.get('feature_recommendations', [])[:3], 1):
                  footer += f'\\n{i}. **{feature.get('feature', 'Unknown').replace('_', ' ').title()}** (demand: {feature.get('demand_score', 0):.1f})'
              
              footer += '\\n\\n*Last updated: Generated from automated feedback analysis pipeline*'
              
              # Remove old analysis section if it exists
              import re
              content = re.sub(r'---\\s*## ðŸ¤– Latest Analysis Summary.*$', '', content, flags=re.DOTALL)
              
              # Add new analysis section
              content = content.strip() + footer
              
              # Write updated messaging
              with open('$MESSAGING_FILE', 'w') as f:
                  f.write(content)
              
              print('âœ… Messaging updated with latest insights')
              
          except Exception as e:
              print(f'âš ï¸ Failed to update messaging: {e}')
          "
          
          echo "messaging_updated=true" >> $GITHUB_OUTPUT
          echo "ðŸ“Š Marketing messaging updated with latest analysis"
      
      # STEP 11: COMMIT CHANGES
      # Commit all generated files back to the repository with detailed commit message
      - name: ðŸ’¾ Commit Generated Files
        id: commit_changes
        run: |
          echo "ðŸ“¤ Committing generated files..."
          
          # Configure git for automated commits
          git config --local user.email "action@github.com"
          git config --local user.name "Lead Magnet Automation"
          
          # Add all relevant files
          git add \
            "$DATA_DIR"/*.json \
            "$CHECKLISTS_DIR"/*pdf \
            "$CHECKLISTS_DIR"/*txt \
            "$OUTPUT_DIR"/*.csv \
            ./messaging.md \
            || true
          
          # Check if there are changes to commit
          if git diff --staged --quiet; then
            echo "ðŸ“ No changes to commit"
            echo "files_committed=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # Create detailed commit message
          COMMIT_MESSAGE="ðŸŽ¯ Automated lead magnet generation $(date -u +%Y-%m-%d)"
          
          # Add metrics to commit message
          if [ -n "${{ steps.reddit_extraction.outputs.pain_points_count }}" ]; then
            COMMIT_MESSAGE="$COMMIT_MESSAGE
          
          ðŸ“Š GENERATION SUMMARY:
          â€¢ Reddit pain points: ${{ steps.reddit_extraction.outputs.pain_points_count }}
          â€¢ Generated checklists: ${{ steps.checklist_generation.outputs.checklist_count }}  
          â€¢ Actionable insights: ${{ steps.feedback_analysis.outputs.insights_count }}
          â€¢ Analysis date: $(date -u +%Y-%m-%d\ %H:%M\ UTC)
          
          ðŸ¤– Generated by automated workflow
          Keywords: $REDDIT_KEYWORDS
          Subreddits: $TARGET_SUBREDDITS"
          fi
          
          # Commit with detailed message
          git commit -m "$COMMIT_MESSAGE"
          
          # Push changes
          for attempt in {1..3}; do
            echo "ðŸ“¤ Push attempt $attempt/3..."
            if git push; then
              echo "âœ… Changes committed and pushed successfully"
              echo "files_committed=true" >> $GITHUB_OUTPUT
              break
            elif [ $attempt -eq 3 ]; then
              echo "âŒ Failed to push after 3 attempts"
              echo "files_committed=false" >> $GITHUB_OUTPUT
              exit 1
            else
              echo "ðŸ”„ Push failed, retrying in 10 seconds..."
              sleep 10
            fi
          done
      
      # STEP 12: SLACK NOTIFICATION
      # Send comprehensive status update to Slack channel
      - name: ðŸ“¢ Send Slack Notification
        if: env.SKIP_NOTIFICATIONS != 'true' && (success() || failure())
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          channel: '#automation'
          username: 'Lead Magnet Bot'
          icon_emoji: ':robot_face:'
          title: 'ðŸŽ¯ Lead Magnet Automation Complete'
          message: |
            **Status:** ${{ job.status == 'success' && 'âœ… Success' || 'âŒ Failed' }}
            **Duration:** ${{ steps.commit_changes.conclusion != 'skipped' && '~25 minutes' || '~15 minutes' }}
            
            **ðŸ“Š Generation Results:**
            â€¢ Reddit Pain Points: ${{ steps.reddit_extraction.outputs.pain_points_count || 'N/A' }}
            â€¢ PDF Checklists: ${{ steps.checklist_generation.outputs.checklist_count || 'N/A' }}
            â€¢ Insights Generated: ${{ steps.feedback_analysis.outputs.insights_count || 'N/A' }}
            â€¢ Files Committed: ${{ steps.commit_changes.outputs.files_committed || 'false' }}
            
            **ðŸ” Data Sources:**
            â€¢ Keywords: `${{ env.REDDIT_KEYWORDS }}`
            â€¢ Subreddits: `${{ env.TARGET_SUBREDDITS }}`
            â€¢ Cache Used: ${{ steps.reddit_extraction.outputs.reddit_cached || 'false' }}
            
            **ðŸ“ Generated Files:**
            â€¢ Pain points data: `/data/reddit_pain_points.json`
            â€¢ Checklists: `/public/downloads/checklists/`
            â€¢ Analysis: `/data/feedback_analysis.json`
            â€¢ CSV exports: `/data/exports/`
            
            **ðŸ”— Repository:** ${{ github.repository }}
            **âš¡ Workflow:** ${{ github.workflow }}
            **ðŸ”„ Run:** #${{ github.run_number }}
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
      
      # STEP 13: DISCORD NOTIFICATION
      # Send status update to Discord channel with rich embed
      - name: ðŸŽ® Send Discord Notification
        if: env.SKIP_NOTIFICATIONS != 'true' && (success() || failure())
        run: |
          if [ -z "${{ secrets.DISCORD_WEBHOOK_URL }}" ]; then
            echo "âš ï¸ Discord webhook URL not configured, skipping notification"
            exit 0
          fi
          
          # Determine status color and emoji
          if [ "${{ job.status }}" = "success" ]; then
            COLOR="5763719"  # Green
            STATUS_EMOJI="âœ…"
            STATUS_TEXT="Success"
          else
            COLOR="15548997"  # Red
            STATUS_EMOJI="âŒ"
            STATUS_TEXT="Failed"
          fi
          
          # Create Discord embed payload
          curl -H "Content-Type: application/json" -d "{
            \"embeds\": [{
              \"title\": \"ðŸŽ¯ Lead Magnet Automation Complete\",
              \"description\": \"**Status:** $STATUS_EMOJI $STATUS_TEXT\",
              \"color\": $COLOR,
              \"fields\": [
                {
                  \"name\": \"ðŸ“Š Generation Results\",
                  \"value\": \"â€¢ Reddit Pain Points: \`${{ steps.reddit_extraction.outputs.pain_points_count || 'N/A' }}\`\\nâ€¢ PDF Checklists: \`${{ steps.checklist_generation.outputs.checklist_count || 'N/A' }}\`\\nâ€¢ Insights Generated: \`${{ steps.feedback_analysis.outputs.insights_count || 'N/A' }}\`\\nâ€¢ Files Committed: \`${{ steps.commit_changes.outputs.files_committed || 'false' }}\`\",
                  \"inline\": true
                },
                {
                  \"name\": \"ðŸ” Data Sources\",
                  \"value\": \"â€¢ Keywords: \`$REDDIT_KEYWORDS\`\\nâ€¢ Cache Used: \`${{ steps.reddit_extraction.outputs.reddit_cached || 'false' }}\`\\nâ€¢ Workflow Run: \`#${{ github.run_number }}\`\",
                  \"inline\": true
                },
                {
                  \"name\": \"ðŸ”— Repository\",
                  \"value\": \"[\`${{ github.repository }}\`](https://github.com/${{ github.repository }})\",
                  \"inline\": false
                }
              ],
              \"footer\": {
                \"text\": \"Lead Magnet Automation â€¢ $(date -u +'%Y-%m-%d %H:%M UTC')\"
              }
            }]
          }" "${{ secrets.DISCORD_WEBHOOK_URL }}"
          
          echo "ðŸ“¬ Discord notification sent"
      
      # STEP 14: WORKFLOW SUMMARY
      # Generate comprehensive summary for GitHub Actions UI
      - name: ðŸ“‹ Generate Workflow Summary
        if: always()
        run: |
          echo "## ðŸŽ¯ Lead Magnet Automation Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Status overview
          if [ "${{ job.status }}" = "success" ]; then
            echo "### âœ… Status: **Success**" >> $GITHUB_STEP_SUMMARY
          else
            echo "### âŒ Status: **Failed**" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Generation metrics
          echo "### ðŸ“Š Generation Metrics" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Reddit Pain Points | ${{ steps.reddit_extraction.outputs.pain_points_count || 'N/A' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Generated Checklists | ${{ steps.checklist_generation.outputs.checklist_count || 'N/A' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Actionable Insights | ${{ steps.feedback_analysis.outputs.insights_count || 'N/A' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Files Committed | ${{ steps.commit_changes.outputs.files_committed || 'false' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Cache Used | ${{ steps.reddit_extraction.outputs.reddit_cached || 'false' }} |" >> $GITHUB_STEP_SUMMARY
          
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Configuration details
          echo "### âš™ï¸ Configuration" >> $GITHUB_STEP_SUMMARY
          echo "- **Keywords**: \`$REDDIT_KEYWORDS\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Subreddits**: \`$TARGET_SUBREDDITS\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Force Regenerate**: \`$FORCE_REGENERATE\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Notifications**: ${{ env.SKIP_NOTIFICATIONS == 'true' && 'Disabled' || 'Enabled' }}" >> $GITHUB_STEP_SUMMARY
          
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Next steps
          echo "### ðŸš€ Next Steps" >> $GITHUB_STEP_SUMMARY
          echo "1. **Review generated checklists** in \`/public/downloads/checklists/\`" >> $GITHUB_STEP_SUMMARY
          echo "2. **Check analysis insights** in \`/data/feedback_analysis.json\`" >> $GITHUB_STEP_SUMMARY  
          echo "3. **Update marketing copy** based on messaging recommendations" >> $GITHUB_STEP_SUMMARY
          echo "4. **Deploy checklists** to production environment" >> $GITHUB_STEP_SUMMARY
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "*Generated by Lead Magnet Automation â€¢ Run #${{ github.run_number }} â€¢ $(date -u +'%Y-%m-%d %H:%M UTC')*" >> $GITHUB_STEP_SUMMARY