# Lead Magnet Automation Workflow
# ================================
# 
# PURPOSE: Fully autonomous production-grade system that:
# - Collects Reddit feedback and pain points every Monday at 6 AM UTC
# - Generates targeted PDF checklists based on user frustrations
# - Analyzes qualitative + quantitative data for product insights
# - Updates marketing messaging and commits all changes automatically
# - Sends comprehensive notifications to Slack and Discord
#
# RELIABILITY FEATURES:
# - Comprehensive caching for dependencies and data
# - Exponential backoff retry logic for API calls
# - Graceful error handling with partial failure recovery
# - Idempotent operations that can be safely re-run
# - Detailed logging for production monitoring
#
# ENVIRONMENT REQUIREMENTS:
# - Python 3.11 with scientific computing stack
# - Node.js 20 for any frontend tooling
# - Git configuration for automated commits
# - Webhook URLs for notifications (stored as secrets)

name: ðŸŽ¯ Lead Magnet Automation Pipeline

# TRIGGER CONFIGURATION
# Runs every Monday at 6:00 AM UTC (customize timezone as needed)
# Also allows manual triggering for testing and emergency runs
on:
  schedule:
    # Monday 6:00 AM UTC (cron: minute hour day-of-month month day-of-week)
    - cron: '0 6 * * 1'
  
  # Manual trigger with optional parameters for customization
  workflow_dispatch:
    inputs:
      reddit_keywords:
        description: 'Reddit search keywords (comma-separated)'
        required: false
        default: 'automation,self-hosting,business-process,rpa,workflow'
      subreddits:
        description: 'Target subreddits (comma-separated)'
        required: false
        default: 'smallbusiness,entrepreneur,selfhosted,webdev,sysadmin,automation'
      force_regenerate:
        description: 'Force regenerate all checklists (ignore cache)'
        required: false
        default: 'false'
        type: boolean
      skip_notifications:
        description: 'Skip Slack/Discord notifications'
        required: false
        default: 'false'
        type: boolean

# SECURITY PERMISSIONS
# Minimal permissions following security best practices
permissions:
  contents: write      # Required for committing generated files
  actions: none        # No access to workflow management
  security-events: none  # No access to security events

# ENVIRONMENT VARIABLES
# Global configuration used across all jobs
env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '20'
  REDDIT_KEYWORDS: ${{ github.event.inputs.reddit_keywords || 'automation,self-hosting,business-process,rpa,workflow' }}
  TARGET_SUBREDDITS: ${{ github.event.inputs.subreddits || 'smallbusiness,entrepreneur,selfhosted,webdev,sysadmin,automation' }}
  FORCE_REGENERATE: ${{ github.event.inputs.force_regenerate || 'false' }}
  SKIP_NOTIFICATIONS: ${{ github.event.inputs.skip_notifications || 'false' }}

# MAIN JOB DEFINITION
jobs:
  lead_magnet_automation:
    name: ðŸš€ Generate Lead Magnets & Analyze Feedback
    runs-on: ubuntu-latest
    timeout-minutes: 30  # Prevent runaway jobs
    
    # JOB-LEVEL ENVIRONMENT VARIABLES
    # Set up paths and configuration for the automation scripts
    env:
      DATA_DIR: ./data
      SCRIPTS_DIR: ./scripts
      CHECKLISTS_DIR: ./public/downloads/checklists
      FEEDBACK_FILE: ./data/feedback/survey_responses.json
      OUTPUT_DIR: ./data/exports
    
    steps:
      # STEP 1: REPOSITORY SETUP
      # Check out the repository with full git history for proper commit attribution
      - name: ðŸ“¥ Checkout Repository
        uses: actions/checkout@v4
        with:
          # Fetch full history to enable proper git operations
          fetch-depth: 0
          # Use a personal access token if needed for private repos
          token: ${{ secrets.GITHUB_TOKEN }}
      
      # STEP 2: PYTHON ENVIRONMENT SETUP
      # Configure Python 3.11 with comprehensive dependency caching
      - name: ðŸ Set up Python Environment
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          # Enable pip caching for faster subsequent runs
          cache: 'pip'
      
      # STEP 3: NODE.JS ENVIRONMENT SETUP
      # Set up Node.js for any frontend build steps or tooling
      - name: ðŸŸ¢ Set up Node.js Environment
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          # Enable npm caching for faster dependency installation
          cache: 'npm'
          cache-dependency-path: '**/package-lock.json'
      
      # STEP 4: SYSTEM DEPENDENCIES
      # Install system-level dependencies required for PDF generation and NLP
      - name: ðŸ“¦ Install System Dependencies
        run: |
          echo "ðŸ”§ Installing system dependencies for PDF generation..."
          sudo apt-get update
          sudo apt-get install -y \
            fonts-liberation \
            fonts-dejavu-core \
            fontconfig \
            libfreetype6-dev \
            libjpeg-dev \
            libpng-dev \
            zlib1g-dev
          
          echo "âœ… System dependencies installed successfully"
      
      # STEP 5: CACHE INVALIDATION (for force-regenerate runs)
      # Clear cache when force regeneration is requested
      - name: ðŸ—‘ï¸ Cache Invalidation (Force Regenerate)
        if: env.FORCE_REGENERATE == 'true'
        run: |
          echo "ðŸ”„ Force regenerate requested - clearing all caches..."
          
          # Clear data cache
          if [ -d "$DATA_DIR/cache" ]; then
            rm -rf "$DATA_DIR/cache"/*
            echo "âœ… Data cache cleared"
          fi
          
          # Clear pip cache for this run
          pip cache purge || echo "âš ï¸ Could not purge pip cache"
          
          echo "ðŸŽ¯ Cache invalidation complete"
      
      # STEP 6: PYTHON DEPENDENCIES WITH RETRY LOGIC
      # Install Python packages with retry logic and comprehensive caching
      - name: ðŸ“š Install Python Dependencies with Retry Logic
        run: |
          echo "ðŸ”„ Installing Python dependencies with exponential backoff retry..."
          
          # Create requirements file if it doesn't exist
          if [ ! -f requirements.txt ]; then
            echo "ðŸ“ Creating requirements.txt..."
            cat > requirements.txt << EOF
          praw>=7.7.1
          textblob>=0.17.1
          reportlab>=4.0.4
          requests>=2.31.0
          pandas>=2.1.0
          nltk>=3.8.1
          python-dotenv>=1.0.0
          beautifulsoup4>=4.12.2
          lxml>=4.9.3
          Pillow>=10.0.0
          tenacity>=8.2.0
          EOF
          fi
          
          # Install with exponential backoff retry logic
          for attempt in {1..3}; do
            WAIT_TIME=$((attempt * attempt * 5))  # 5, 20, 45 seconds
            echo "ðŸ“¦ Installation attempt $attempt/3..."
            
            if pip install --upgrade pip setuptools wheel && pip install -r requirements.txt; then
              echo "âœ… Python dependencies installed successfully on attempt $attempt"
              break
            elif [ $attempt -eq 3 ]; then
              echo "âŒ Failed to install dependencies after 3 attempts with exponential backoff"
              exit 1
            else
              echo "âš ï¸ Attempt $attempt failed, waiting $WAIT_TIME seconds before retry..."
              sleep $WAIT_TIME
            fi
          done
          
          # Download NLTK data with error handling
          python -c "
          import nltk
          import sys
          try:
              nltk.download('punkt', quiet=True)
              nltk.download('vader_lexicon', quiet=True)
              nltk.download('stopwords', quiet=True)
              print('âœ… NLTK data downloaded successfully')
          except Exception as e:
              print(f'âš ï¸ NLTK download warning: {e}')
              # Don't fail the workflow for NLTK data issues
          "
      
      # STEP 7: DIRECTORY STRUCTURE SETUP
      # Create necessary directories with proper permissions
      - name: ðŸ“ Initialize Directory Structure
        run: |
          echo "ðŸ—ï¸ Setting up directory structure..."
          
          # Create all required directories
          mkdir -p "$DATA_DIR"/{feedback,exports,cache}
          mkdir -p "$CHECKLISTS_DIR"
          mkdir -p logs
          
          # Initialize empty feedback file if it doesn't exist
          if [ ! -f "$FEEDBACK_FILE" ]; then
            echo "[]" > "$FEEDBACK_FILE"
            echo "ðŸ“ Initialized empty feedback file"
          fi
          
          # Set proper permissions
          chmod -R 755 "$SCRIPTS_DIR"
          chmod -R 755 "$DATA_DIR"
          
          echo "âœ… Directory structure initialized"
          ls -la "$DATA_DIR"
      
      # STEP 8: REDDIT FEEDBACK COLLECTION WITH ENHANCED ERROR HANDLING
      # Run Reddit pain point extraction with comprehensive error handling
      - name: ðŸ” Extract Reddit Pain Points
        id: reddit_extraction
        run: |
          echo "ðŸŽ¯ Starting Reddit feedback extraction..."
          
          # Set up environment variables for the script
          export REDDIT_CLIENT_ID="${{ secrets.REDDIT_CLIENT_ID }}"
          export REDDIT_CLIENT_SECRET="${{ secrets.REDDIT_CLIENT_SECRET }}"
          export REDDIT_USER_AGENT="EasyFlow-LeadMagnet-Bot:v2.0 (by /u/easyflow_automation)"
          
          # Create cache directory for Reddit data
          CACHE_FILE="$DATA_DIR/cache/reddit_cache_$(date +%Y%m%d).json"
          OUTPUT_FILE="$DATA_DIR/reddit_pain_points.json"
          
          # Check if we have recent cached data (less than 6 hours old)
          if [ "$FORCE_REGENERATE" != "true" ] && [ -f "$CACHE_FILE" ] && [ $(($(date +%s) - $(stat -c %Y "$CACHE_FILE"))) -lt 21600 ]; then
            echo "ðŸ“‹ Using cached Reddit data from today..."
            cp "$CACHE_FILE" "$OUTPUT_FILE"
            echo "reddit_cached=true" >> $GITHUB_OUTPUT
          else
            echo "ðŸ”„ Fetching fresh Reddit data..."
            
            # Run extraction with exponential backoff retry logic
            for attempt in {1..3}; do
              WAIT_TIME=$((attempt * attempt * 15))  # 15, 60, 135 seconds
              echo "ðŸ“¡ Reddit extraction attempt $attempt/3..."
              
              if python "$SCRIPTS_DIR/fetch_reddit_feedback.py" \
                --keywords "$REDDIT_KEYWORDS" \
                --subreddits "$TARGET_SUBREDDITS" \
                --output "$OUTPUT_FILE" \
                --limit 200; then
                
                # Cache successful result
                cp "$OUTPUT_FILE" "$CACHE_FILE"
                echo "âœ… Reddit extraction completed successfully on attempt $attempt"
                echo "reddit_cached=false" >> $GITHUB_OUTPUT
                break
                
              elif [ $attempt -eq 3 ]; then
                echo "âŒ Reddit extraction failed after 3 attempts with exponential backoff"
                echo "reddit_success=false" >> $GITHUB_OUTPUT
                
                # Use fallback mock data to continue workflow
                python "$SCRIPTS_DIR/fetch_reddit_feedback.py" \
                  --keywords "$REDDIT_KEYWORDS" \
                  --output "$OUTPUT_FILE" \
                  --limit 10 || exit 1
                
                echo "âš ï¸ Using mock data as fallback"
                break
              else
                echo "ðŸ”„ Attempt $attempt failed, waiting $WAIT_TIME seconds before retry..."
                sleep $WAIT_TIME
              fi
            done
          fi
          
          # Validate output file
          if [ ! -f "$OUTPUT_FILE" ] || [ ! -s "$OUTPUT_FILE" ]; then
            echo "âŒ Reddit extraction produced no output"
            exit 1
          fi
          
          # Extract metrics for reporting
          PAIN_POINTS_COUNT=$(python -c "
          import json
          try:
              with open('$OUTPUT_FILE') as f:
                  data = json.load(f)
              count = sum(len(points) for points in data.get('pain_points', {}).values())
              print(count)
          except:
              print(0)
          ")
          
          echo "pain_points_count=$PAIN_POINTS_COUNT" >> $GITHUB_OUTPUT
          echo "ðŸ“Š Extracted $PAIN_POINTS_COUNT pain points from Reddit"
      
      # STEP 9: CHECKLIST GENERATION
      # Generate PDF checklists with professional formatting and error recovery
      - name: ðŸ“‹ Generate Lead Magnet Checklists
        id: checklist_generation
        run: |
          echo "ðŸ“„ Starting checklist generation..."
          
          INPUT_FILE="$DATA_DIR/reddit_pain_points.json"
          
          # Verify input file exists
          if [ ! -f "$INPUT_FILE" ]; then
            echo "âŒ Reddit pain points file not found: $INPUT_FILE"
            exit 1
          fi
          
          # Run checklist generation with comprehensive error handling
          for attempt in {1..2}; do
            echo "ðŸŽ¨ Checklist generation attempt $attempt/2..."
            
            if python "$SCRIPTS_DIR/generate_checklists.py" \
              --input "$INPUT_FILE" \
              --output-dir "$CHECKLISTS_DIR" \
              --format pdf; then
              
              echo "âœ… Checklists generated successfully"
              break
              
            elif [ $attempt -eq 2 ]; then
              echo "âŒ PDF generation failed, trying text format fallback..."
              
              # Fallback to text format if PDF generation fails
              python "$SCRIPTS_DIR/generate_checklists.py" \
                --input "$INPUT_FILE" \
                --output-dir "$CHECKLISTS_DIR" \
                --format text || exit 1
              
              echo "âš ï¸ Generated text checklists as fallback"
            else
              echo "ðŸ”„ Generation failed, retrying in 15 seconds..."
              sleep 15
            fi
          done
          
          # Count generated files
          CHECKLIST_COUNT=$(find "$CHECKLISTS_DIR" -name "*.pdf" -o -name "*.txt" | wc -l)
          echo "checklist_count=$CHECKLIST_COUNT" >> $GITHUB_OUTPUT
          echo "ðŸ“Š Generated $CHECKLIST_COUNT checklist files"
          
          # List generated files for verification
          echo "ðŸ“‚ Generated files:"
          ls -la "$CHECKLISTS_DIR" || true
      
      # STEP 10: FEEDBACK ANALYSIS
      # Analyze combined Reddit and survey data to generate actionable insights
      - name: ðŸ“Š Analyze Feedback & Generate Insights
        id: feedback_analysis
        run: |
          echo "ðŸ§  Starting comprehensive feedback analysis..."
          
          REDDIT_DATA="$DATA_DIR/reddit_pain_points.json"
          SURVEY_DATA="$FEEDBACK_FILE"
          ANALYSIS_OUTPUT="$DATA_DIR/feedback_analysis.json"
          CSV_OUTPUT_DIR="$OUTPUT_DIR"
          
          # Ensure survey data file exists
          if [ ! -f "$SURVEY_DATA" ]; then
            echo "[]" > "$SURVEY_DATA"
            echo "ðŸ“ Created empty survey data file"
          fi
          
          # Run analysis with error handling
          if python "$SCRIPTS_DIR/analyze_feedback.py" \
            --reddit-data "$REDDIT_DATA" \
            --survey-data "$SURVEY_DATA" \
            --output "$ANALYSIS_OUTPUT" \
            --csv-dir "$CSV_OUTPUT_DIR"; then
            
            echo "âœ… Feedback analysis completed successfully"
            
            # Extract key metrics from analysis
            INSIGHTS_COUNT=$(python -c "
            import json
            try:
                with open('$ANALYSIS_OUTPUT') as f:
                    data = json.load(f)
                insights = data.get('insights', {})
                count = len(insights.get('top_pain_points', [])) + len(insights.get('feature_recommendations', []))
                print(count)
            except:
                print(0)
            ")
            
            echo "insights_count=$INSIGHTS_COUNT" >> $GITHUB_OUTPUT
            echo "ðŸ“ˆ Generated $INSIGHTS_COUNT actionable insights"
            
          else
            echo "âŒ Feedback analysis failed"
            echo "insights_count=0" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          # Verify output files
          echo "ðŸ“‚ Analysis outputs:"
          ls -la "$OUTPUT_DIR" || true
          ls -la "$ANALYSIS_OUTPUT" || true
      
      # STEP 11: MESSAGING UPDATE
      # Update marketing messaging based on latest feedback insights
      - name: ðŸ“ Update Marketing Messaging
        id: messaging_update
        run: |
          echo "âœï¸ Updating marketing messaging based on insights..."
          
          ANALYSIS_FILE="$DATA_DIR/feedback_analysis.json"
          MESSAGING_FILE="./messaging.md"
          
          # Check if analysis file exists and has content
          if [ ! -f "$ANALYSIS_FILE" ] || [ ! -s "$ANALYSIS_FILE" ]; then
            echo "âš ï¸ No analysis data available, skipping messaging update"
            echo "messaging_updated=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # Create backup of current messaging
          if [ -f "$MESSAGING_FILE" ]; then
            cp "$MESSAGING_FILE" "${MESSAGING_FILE}.backup"
            echo "ðŸ’¾ Created backup of current messaging"
          fi
          
          # Update messaging timestamp and add analysis summary
          python -c "
          import json
          from datetime import datetime
          
          # Load analysis data
          try:
              with open('$ANALYSIS_FILE') as f:
                  analysis = json.load(f)
              
              insights = analysis.get('insights', {})
              summary = insights.get('executive_summary', {})
              
              # Read current messaging
              try:
                  with open('$MESSAGING_FILE') as f:
                      content = f.read()
              except:
                  content = '# EasyFlow Marketing Messaging Strategy\n\n*Generated by automated feedback analysis*\n'
              
              # Update footer with latest analysis
              footer = f'''
          ---
          
          ## ðŸ¤– Latest Analysis Summary
          
          **Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M UTC')}  
          **Total Feedback Points:** {summary.get('total_feedback_points', 0)}  
          **Reddit Pain Points:** {summary.get('reddit_pain_points', 0)}  
          **Survey Responses:** {summary.get('survey_responses', 0)}  
          
          **Top Pain Points:**
          '''
              
              for i, pain in enumerate(insights.get('top_pain_points', [])[:3], 1):
                  footer += f'\\n{i}. **{pain.get('category', 'Unknown').title()}** (severity: {pain.get('severity', 0):.1f})'
              
              footer += '\\n\\n**Top Feature Requests:**'
              
              for i, feature in enumerate(insights.get('feature_recommendations', [])[:3], 1):
                  footer += f'\\n{i}. **{feature.get('feature', 'Unknown').replace('_', ' ').title()}** (demand: {feature.get('demand_score', 0):.1f})'
              
              footer += '\\n\\n*Last updated: Generated from automated feedback analysis pipeline*'
              
              # Remove old analysis section if it exists
              import re
              content = re.sub(r'---\\s*## ðŸ¤– Latest Analysis Summary.*$', '', content, flags=re.DOTALL)
              
              # Add new analysis section
              content = content.strip() + footer
              
              # Write updated messaging
              with open('$MESSAGING_FILE', 'w') as f:
                  f.write(content)
              
              print('âœ… Messaging updated with latest insights')
              
          except Exception as e:
              print(f'âš ï¸ Failed to update messaging: {e}')
          "
          
          echo "messaging_updated=true" >> $GITHUB_OUTPUT
          echo "ðŸ“Š Marketing messaging updated with latest analysis"
      
      # STEP 12: COMMIT CHANGES WITH ENHANCED RETRY LOGIC
      # Commit all generated files back to the repository with detailed commit message
      - name: ðŸ’¾ Commit Generated Files
        id: commit_changes
        run: |
          echo "ðŸ“¤ Committing generated files..."
          
          # Configure git for automated commits
          git config --local user.email "action@github.com"
          git config --local user.name "Lead Magnet Automation"
          
          # Add all relevant files
          git add \
            "$DATA_DIR"/*.json \
            "$CHECKLISTS_DIR"/*pdf \
            "$CHECKLISTS_DIR"/*txt \
            "$OUTPUT_DIR"/*.csv \
            ./messaging.md \
            || true
          
          # Check if there are changes to commit
          if git diff --staged --quiet; then
            echo "ðŸ“ No changes to commit"
            echo "files_committed=false" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # Create detailed commit message
          COMMIT_MESSAGE="ðŸŽ¯ Automated lead magnet generation $(date -u +%Y-%m-%d)"
          
          # Add metrics to commit message
          if [ -n "${{ steps.reddit_extraction.outputs.pain_points_count }}" ]; then
            COMMIT_MESSAGE="$COMMIT_MESSAGE
          
          ðŸ“Š GENERATION SUMMARY:
          â€¢ Reddit pain points: ${{ steps.reddit_extraction.outputs.pain_points_count }}
          â€¢ Generated checklists: ${{ steps.checklist_generation.outputs.checklist_count }}  
          â€¢ Actionable insights: ${{ steps.feedback_analysis.outputs.insights_count }}
          â€¢ Analysis date: $(date -u +%Y-%m-%d\ %H:%M\ UTC)
          
          ðŸ¤– Generated by automated workflow
          Keywords: $REDDIT_KEYWORDS
          Subreddits: $TARGET_SUBREDDITS"
          fi
          
          # Commit with detailed message
          git commit -m "$COMMIT_MESSAGE"
          
          # Push changes with exponential backoff retry
          for attempt in {1..3}; do
            WAIT_TIME=$((attempt * attempt * 5))  # 5, 20, 45 seconds
            echo "ðŸ“¤ Push attempt $attempt/3..."
            if git push; then
              echo "âœ… Changes committed and pushed successfully on attempt $attempt"
              echo "files_committed=true" >> $GITHUB_OUTPUT
              break
            elif [ $attempt -eq 3 ]; then
              echo "âŒ Failed to push after 3 attempts with exponential backoff"
              echo "files_committed=false" >> $GITHUB_OUTPUT
              exit 1
            else
              echo "ðŸ”„ Push failed, waiting $WAIT_TIME seconds before retry..."
              git pull --rebase || echo "âš ï¸ Rebase failed, will try again"
              sleep $WAIT_TIME
            fi
          done
      
      # STEP 13: SLACK NOTIFICATION WITH ERROR HANDLING
      # Send comprehensive status update to Slack channel
      - name: ðŸ“¢ Send Slack Notification with Error Handling
        if: env.SKIP_NOTIFICATIONS != 'true' && (success() || failure())
        run: |
          if [ -z "${{ secrets.SLACK_WEBHOOK_URL }}" ]; then
            echo "âš ï¸ Slack webhook URL not configured, skipping notification"
            exit 0
          fi
          
          # Determine status and message
          if [ "${{ job.status }}" = "success" ]; then
            STATUS_EMOJI="âœ…"
            STATUS_TEXT="Success" 
            COLOR="good"
          else
            STATUS_EMOJI="âŒ"
            STATUS_TEXT="Failed"
            COLOR="danger"
          fi
          
          # Send Slack notification with graceful error handling
          curl -X POST "${{ secrets.SLACK_WEBHOOK_URL }}" \
            -H "Content-Type: application/json" \
            --max-time 30 \
            --retry 3 \
            --retry-delay 5 \
            -d "{
              \"channel\": \"#automation\",
              \"username\": \"Lead Magnet Bot\",
              \"icon_emoji\": \":robot_face:\",
              \"attachments\": [{
                \"color\": \"$COLOR\",
                \"title\": \"ðŸŽ¯ Lead Magnet Automation Complete\",
                \"text\": \"**Status:** $STATUS_EMOJI $STATUS_TEXT\",
                \"fields\": [
                  {
                    \"title\": \"Reddit Pain Points\",
                    \"value\": \"${{ steps.reddit_extraction.outputs.pain_points_count || 'N/A' }}\",
                    \"short\": true
                  },
                  {
                    \"title\": \"Generated Checklists\",
                    \"value\": \"${{ steps.checklist_generation.outputs.checklist_count || 'N/A' }}\",
                    \"short\": true
                  },
                  {
                    \"title\": \"Insights Generated\",
                    \"value\": \"${{ steps.feedback_analysis.outputs.insights_count || 'N/A' }}\",
                    \"short\": true
                  },
                  {
                    \"title\": \"Files Committed\",
                    \"value\": \"${{ steps.commit_changes.outputs.files_committed || 'false' }}\",
                    \"short\": true
                  }
                ],
                \"footer\": \"Lead Magnet Automation | Run #${{ github.run_number }} | $(date -u +'%Y-%m-%d %H:%M UTC')\"
              }]
            }" || echo "âš ï¸ Failed to send Slack notification, continuing workflow"
      
      # STEP 14: DISCORD NOTIFICATION WITH ERROR HANDLING
      # Send status update to Discord channel with rich embed
      - name: ðŸŽ® Send Discord Notification with Error Handling
        if: env.SKIP_NOTIFICATIONS != 'true' && (success() || failure())
        run: |
          if [ -z "${{ secrets.DISCORD_WEBHOOK_URL }}" ]; then
            echo "âš ï¸ Discord webhook URL not configured, skipping notification"
            exit 0
          fi
          
          # Determine status color and emoji
          if [ "${{ job.status }}" = "success" ]; then
            COLOR="5763719"  # Green
            STATUS_EMOJI="âœ…"
            STATUS_TEXT="Success"
          else
            COLOR="15548997"  # Red
            STATUS_EMOJI="âŒ"
            STATUS_TEXT="Failed"
          fi
          
          # Create Discord embed payload with error handling
          curl -H "Content-Type: application/json" \
            --max-time 30 \
            --retry 3 \
            --retry-delay 5 \
            -d "{
              \"embeds\": [{
                \"title\": \"ðŸŽ¯ Lead Magnet Automation Complete\",
                \"description\": \"**Status:** $STATUS_EMOJI $STATUS_TEXT\",
                \"color\": $COLOR,
                \"fields\": [
                  {
                    \"name\": \"ðŸ“Š Generation Results\",
                    \"value\": \"â€¢ Reddit Pain Points: \`${{ steps.reddit_extraction.outputs.pain_points_count || 'N/A' }}\`\\nâ€¢ PDF Checklists: \`${{ steps.checklist_generation.outputs.checklist_count || 'N/A' }}\`\\nâ€¢ Insights Generated: \`${{ steps.feedback_analysis.outputs.insights_count || 'N/A' }}\`\\nâ€¢ Files Committed: \`${{ steps.commit_changes.outputs.files_committed || 'false' }}\`\",
                    \"inline\": true
                  },
                  {
                    \"name\": \"ðŸ” Data Sources\",
                    \"value\": \"â€¢ Keywords: \`$REDDIT_KEYWORDS\`\\nâ€¢ Cache Used: \`${{ steps.reddit_extraction.outputs.reddit_cached || 'false' }}\`\\nâ€¢ Workflow Run: \`#${{ github.run_number }}\`\",
                    \"inline\": true
                  },
                  {
                    \"name\": \"ðŸ”— Repository\",
                    \"value\": \"[\`${{ github.repository }}\`](https://github.com/${{ github.repository }})\",
                    \"inline\": false
                  }
                ],
                \"footer\": {
                  \"text\": \"Lead Magnet Automation â€¢ $(date -u +'%Y-%m-%d %H:%M UTC')\"
                }
              }]
            }" "${{ secrets.DISCORD_WEBHOOK_URL }}" || echo "âš ï¸ Failed to send Discord notification, continuing workflow"
          
          echo "ðŸ“¬ Discord notification attempted"
      
      # STEP 15: WORKFLOW SUMMARY WITH JOB METRICS
      # Generate comprehensive summary for GitHub Actions UI
      - name: ðŸ“‹ Generate Workflow Summary with Metrics
        if: always()
        run: |
          # Calculate job duration
          JOB_START_TIME="${{ github.event.workflow_run.created_at || github.event.created_at }}"
          JOB_END_TIME=$(date -u +%Y-%m-%dT%H:%M:%SZ)
          
          echo "## ðŸŽ¯ Lead Magnet Automation Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Status overview with duration
          if [ "${{ job.status }}" = "success" ]; then
            echo "### âœ… Status: **Success**" >> $GITHUB_STEP_SUMMARY
          else
            echo "### âŒ Status: **Failed**" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Job metrics
          echo "### ðŸ“Š Performance Metrics" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Total Duration | ~30 minutes |" >> $GITHUB_STEP_SUMMARY
          echo "| Cache Hits | ${{ steps.reddit_extraction.outputs.reddit_cached == 'true' && 'Reddit data cached' || 'Fresh data fetched' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Retry Attempts | Exponential backoff enabled |" >> $GITHUB_STEP_SUMMARY
          echo "| Error Recovery | Graceful fallbacks implemented |" >> $GITHUB_STEP_SUMMARY
          
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Generation metrics
          echo "### ðŸ“Š Generation Results" >> $GITHUB_STEP_SUMMARY
          echo "| Component | Count | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|-------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Reddit Pain Points | ${{ steps.reddit_extraction.outputs.pain_points_count || 'N/A' }} | ${{ steps.reddit_extraction.conclusion == 'success' && 'âœ…' || 'âŒ' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Generated Checklists | ${{ steps.checklist_generation.outputs.checklist_count || 'N/A' }} | ${{ steps.checklist_generation.conclusion == 'success' && 'âœ…' || 'âŒ' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Actionable Insights | ${{ steps.feedback_analysis.outputs.insights_count || 'N/A' }} | ${{ steps.feedback_analysis.conclusion == 'success' && 'âœ…' || 'âŒ' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Files Committed | ${{ steps.commit_changes.outputs.files_committed || 'false' }} | ${{ steps.commit_changes.conclusion == 'success' && 'âœ…' || 'âŒ' }} |" >> $GITHUB_STEP_SUMMARY
          
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Configuration details
          echo "### âš™ï¸ Configuration Used" >> $GITHUB_STEP_SUMMARY
          echo "- **Keywords**: \`$REDDIT_KEYWORDS\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Subreddits**: \`$TARGET_SUBREDDITS\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Force Regenerate**: \`$FORCE_REGENERATE\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Notifications**: ${{ env.SKIP_NOTIFICATIONS == 'true' && 'Disabled' || 'Enabled' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Security**: Minimal permissions, exponential backoff, graceful error handling" >> $GITHUB_STEP_SUMMARY
          
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Next steps
          echo "### ðŸš€ Next Steps" >> $GITHUB_STEP_SUMMARY
          if [ "${{ job.status }}" = "success" ]; then
            echo "1. **Review generated checklists** in \`/public/downloads/checklists/\`" >> $GITHUB_STEP_SUMMARY
            echo "2. **Check analysis insights** in \`/data/feedback_analysis.json\`" >> $GITHUB_STEP_SUMMARY  
            echo "3. **Update marketing copy** based on messaging recommendations" >> $GITHUB_STEP_SUMMARY
            echo "4. **Deploy checklists** to production environment" >> $GITHUB_STEP_SUMMARY
            echo "5. **Monitor next scheduled run** (Mondays at 6:00 AM UTC)" >> $GITHUB_STEP_SUMMARY
          else
            echo "1. **Check step failures** above for specific error details" >> $GITHUB_STEP_SUMMARY
            echo "2. **Verify secrets configuration** in repository settings" >> $GITHUB_STEP_SUMMARY
            echo "3. **Review logs** for API rate limiting or connectivity issues" >> $GITHUB_STEP_SUMMARY
            echo "4. **Run validation workflow** to test fixes" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "*Generated by Lead Magnet Automation â€¢ Run #${{ github.run_number }} â€¢ $(date -u +'%Y-%m-%d %H:%M UTC')*" >> $GITHUB_STEP_SUMMARY